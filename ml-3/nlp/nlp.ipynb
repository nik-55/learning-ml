{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463d2a7b-fff7-4fd0-af72-00ae59a67f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1956ef5-0850-431a-bf1c-d48d50e192c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex - Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbac0c3-f49a-481a-873b-4a5ec19a042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"\\d{4}(.*)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7b4649-bf24-4ec3-b769-5d9e061202a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jgkjg54']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern, \"1234jgkjg54\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcda326a-611b-4009-92b7-5621bf575213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://regex101.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa402e8-1071-4356-80f9-1d148027b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9132062-9f8a-47d3-b78f-144fa37c96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df12560-3bbf-4d23-a0f9-abe931c5efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"dr. strange loves pav bhaji etc.. of mumbai. Hulk loves chat of delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be636a16-053f-4b23-aed0-e9d40e878b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr. strange loves pav bhaji etc.. of mumbai.\n",
      "dr\n",
      ".\n",
      "strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "etc\n",
      "..\n",
      "of\n",
      "mumbai\n",
      ".\n",
      "Hulk loves chat of delhi\n",
      "Hulk\n",
      "loves\n",
      "chat\n",
      "of\n",
      "delhi\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for word in sent:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff5e4c3-3ede-4d53-aa89-b6546d5b3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization is the splitting of raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4db06b1f-ff96-4126-90e6-423e761d2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d70aa05-36a8-4308-9abb-e79864f02f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Dr. nikhil loves pav bhaji of mumbai as it costs only 2$ per plate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae88c444-18b3-4933-af91-f35d73cf728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "nikhil\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "2\n",
      "$\n",
      "per\n",
      "plate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d5bce41-a5c4-43e9-b18e-245a33733be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"let's go to mumbai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74ca00d5-813c-4d65-a757-65a7e7a4a2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let\n",
      "'s\n",
      "go\n",
      "to\n",
      "mumbai\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0913871-dd2e-4f14-8271-b689246dd32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_context',\n",
       " '_get_array_attrs',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'copy',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_dict',\n",
       " 'from_disk',\n",
       " 'from_docs',\n",
       " 'from_json',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_annotation',\n",
       " 'has_extension',\n",
       " 'has_unknown_spaces',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_ents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'spans',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_dict',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'to_utf8_array',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69d2aeb0-37d7-4dfa-b506-efe4cd2f8d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "997a7fe1-4c46-4612-9238-a9a0769b2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp( \"this is my email nik.xyz.in@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b58fbb2-f981-4fcc-9b50-cf5d80cbda96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "is\n",
      "my\n",
      "email\n",
      "nik.xyz.in@gmail.com\n",
      "nik.xyz.in@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)\n",
    "    if token.like_email:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5883a59e-1b65-446e-8872-3c811bbe5b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e609a75b-0eb2-4730-9939-bb916a5b32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.attrs import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH:\"gim\"}, \n",
    "    {ORTH:\"me\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c46a35fc-0797-4554-a7ad-c05c43aa6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"gimme some sunshine gimme some faith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2f1e266-8023-4c8f-ae9e-37f2df2a714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gim\n",
      "me\n",
      "some\n",
      "sunshine\n",
      "gim\n",
      "me\n",
      "some\n",
      "faith\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f678768-9c05-4f69-9a22-68708de23b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7ffabc3912d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3481f6e-45ea-4d83-85ff-4523c9bbae7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "751739b7-e16c-45e9-8798-c1cbd2c8aac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gimme some sunshine gimme some faith.\n",
      "gim\n",
      "me\n",
      "some\n",
      "sunshine\n",
      "gim\n",
      "me\n",
      "some\n",
      "faith\n",
      ".\n",
      "gimme the fair of the taxi\n",
      "gim\n",
      "me\n",
      "the\n",
      "fair\n",
      "of\n",
      "the\n",
      "taxi\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"gimme some sunshine gimme some faith. gimme the fair of the taxi\")\n",
    "\n",
    "for token in doc.sents:\n",
    "    print(token)\n",
    "    for word_token in token:\n",
    "        print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "776cb3ed-4b7a-461d-b693-ca3e0815a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer ---> pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "179cf2ff-63ee-4140-8a3a-fb32cb76e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa148bf1-8c43-4281-9890-62dc7bd64b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f704729b-5902-4757-a444-dbbd01fe7a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names, nlp2.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "effcd41d-ddeb-4d83-adb3-aa05c82fcc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"captain america ate 100$ if samosa. then he said I can do this all day.\")\n",
    "doc2 = nlp2(\"captain america ate 100$ if samosa. then he said I can do this all day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7891291d-2179-4a6e-b333-0a3f89bb5c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captain  \n",
      "america  \n",
      "ate  \n",
      "100  \n",
      "$  \n",
      "if  \n",
      "samosa  \n",
      ".  \n",
      "then  \n",
      "he  \n",
      "said  \n",
      "I  \n",
      "can  \n",
      "do  \n",
      "this  \n",
      "all  \n",
      "day  \n",
      ".  \n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82da7fb7-dd1e-4605-b15a-f6c390061762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captain PROPN captain\n",
      "america PROPN america\n",
      "ate VERB eat\n",
      "100 NUM 100\n",
      "$ NUM $\n",
      "if SCONJ if\n",
      "samosa NOUN samosa\n",
      ". PUNCT .\n",
      "then ADV then\n",
      "he PRON he\n",
      "said VERB say\n",
      "I PRON I\n",
      "can AUX can\n",
      "do VERB do\n",
      "this PRON this\n",
      "all DET all\n",
      "day NOUN day\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "246bace3-cf97-426c-bec8-13243a55427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS - Parts of speech - Noun, pronoun, verb\n",
    "# Lemmatization - ate -> eat, gone->go\n",
    "# stemming - remove suffix able, ing - talking -> talk, eatable - eat\n",
    "# ner - named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "812bd9d9-0482-48ca-b614-b1200edff5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f808e970-6044-476a-bb83-84a657712df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla ORG\n",
      "Twitter Inc ORG\n",
      "45 Rs billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# NER \n",
    "\n",
    "doc = nlp2(\"Tesla is going to acquire Twitter Inc for 45 Rs billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "52a40e01-ca0d-42a9-bb72-03f2bc514c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fc9a7548-6dcc-434b-9acd-1415282f552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Representation of words - Vectorization\n",
    "\n",
    "# Label encoding - [1 2 3 4 5]\n",
    "# One hot Encoding - [1 0 0 0 0] [0 1 0 0 0] \n",
    "# Bag of words\n",
    "# TF-IDF\n",
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "87628c4f-d6b8-49ca-a7c9-0858c7ce6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7c849ea4-67ad-4547-8bc3-58f639172489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5a8c8d77-9898-42a9-85cd-fe81286d71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding technique\n",
    "# - word2vec\n",
    "# - fastText\n",
    "# - BERT\n",
    "# - GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3adc28a0-2d93-4118-80bc-7d3fe2db1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f160a-f92d-43f0-bbef-a8ffec23a605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2bd8d-e96d-4676-8a2d-459a3ef8a51b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

import pandas as pd
%matplotlib inline


df = pd.read_csv('data1.csv', delimiter=',')
df.head()


df['smoker'].isna().sum()


df['smoker'].value_counts()


df['smoker'].unique()


df['smoker'].describe()


df['smoker'].count()


df['children'].unique()


df['children'].value_counts()


df['children'].isna().sum()


df['sex'].value_counts()


df['sex'].isna().sum()


df['region'].value_counts()


df['region'].isna().sum()





from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer


x = df.iloc[:,:-1]
y = df.iloc[:,-1]

text_categories = ['region', 'smoker', 'sex']

preprocessor = ColumnTransformer([
    ("encoder", OneHotEncoder(), text_categories),
    ("scaler", StandardScaler(), [col for col in x.columns if col not in text_categories]),
])

pipe = Pipeline(
    [
        ("preprocessor", preprocessor),
        ("lr", LinearRegression())
    ]
)


pipe


from sklearn.model_selection import train_test_split





xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.33, random_state=42)


import seaborn as sns


sns.scatterplot(x='bmi', y='charges', data=df, hue='sex', size='age')


preprocessor.fit_transform(x)


pipe.fit(xtrain,ytrain)


df[['bmi','charges', 'age']].corr()


df['age'].describe()


ypred = pipe.predict(xtest)


ytest.shape


ypred.shape


ytest = ytest.to_numpy()


sns.lineplot(x=ytest.reshape(442), y=ypred.reshape(442))


df.info()


sns.histplot(df['age'])


sns.histplot(df['bmi'])


sns.histplot(x='charges', hue="smoker", data=df)


sns.scatterplot(x="age", y="charges", data=df, hue="smoker")


# weights, bias


import numpy as np


mse = np.mean(np.square(ytest-ypred))
mse


np.sqrt(mse)


pipe.coef_


pipe.named_steps['lr'].coef_


# model, cost, optimizer


sns.barplot(x="smoker", y="charges", data=df)


sns.barplot(x="region", y="charges", data=df)


sns.barplot(x="children", y="charges", data=df)





# logistic regression
# used for binary classification
# 0 to 1 probability prediction z = w0+w1x1+w2x2
# sigmoid function = 1/(1+e^(-z))
# cross entropy loss


# df.dropna(subsets=[])


df.sample(frac=0.1)


# hyperparameter 
# validaion set
# time series data splitting
df.nunique()


# method of filling missing values in the dataset is called imputation
from sklearn.impute import SimpleImputer


SimpleImputer(strategy="mean")


# ?SimpleImputer


# saving the dataset after preprocessing AS parquet or pyarrow


from sklearn.metrics import confusion_matrix


# joblib
# hyperparameter tuning
